{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Project Structure with uv",
        "description": "Set up the Python 3.13 project using uv package manager, create the directory structure, and configure pyproject.toml with all required dependencies and development tools",
        "details": "1. Initialize project with `uv init mover-status-monitor`\n2. Configure pyproject.toml:\n```toml\n[project]\nname = \"mover-status-monitor\"\nversion = \"0.1.0\"\nrequires-python = \">=3.13\"\ndependencies = [\n    \"pydantic>=2.5\",\n    \"pyyaml>=6.0\",\n    \"psutil>=5.9\",\n    \"httpx>=0.25\",\n    \"rich>=13.7\",\n    \"click>=8.1\",\n    \"python-telegram-bot>=20.7\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.4\",\n    \"pytest-cov>=4.1\",\n    \"pytest-asyncio>=0.21\",\n    \"pytest-mock>=3.12\",\n    \"basedpyright>=1.8\",\n    \"ruff>=0.1.9\",\n]\n\n[tool.basedpyright]\ntypeCheckingMode = \"strict\"\nreportMissingImports = true\nreportMissingTypeStubs = false\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\naddopts = \"--cov=src/mover_status --cov-report=term-missing --cov-fail-under=100\"\n```\n3. Create full directory structure as specified in PRD\n4. Add __init__.py files with proper exports\n5. Create LICENSE and README.md files",
        "testStrategy": "Write tests/test_project_structure.py to verify:\n- All directories exist as specified\n- pyproject.toml contains required dependencies\n- Python version is 3.13+\n- basedpyright configuration is strict\n- pytest coverage requirement is 100%",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "UV Project Setup",
            "description": "Initialize a new Python project using the uv package manager, including virtual environment creation and basic project scaffolding",
            "dependencies": [],
            "details": "Run 'uv init' to create the project structure, set up virtual environment with 'uv venv', and activate it. Configure uv.lock file for dependency management.\n<info added on 2025-07-05T16:31:18.479Z>\nTask completed: Successfully ran `uv init --build-backend uv` to initialize the Python project with uv package manager. The project structure has been set up with the uv build backend configuration.\n</info added on 2025-07-05T16:31:18.479Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Pyproject.toml Configuration",
            "description": "Configure the pyproject.toml file with project metadata, dependencies, build system, and development tools",
            "dependencies": [
              1
            ],
            "details": "Define project name, version, description, authors, license, and dependencies. Configure build-system, dev dependencies (pytest, black, flake8, mypy), and tool configurations.\n<info added on 2025-07-05T17:36:11.843Z>\nSuccessfully configured pyproject.toml with comprehensive project metadata and dependencies. Updated project description to match full specifications and added all required runtime dependencies including pydantic, pyyaml, psutil, httpx, rich, click, and python-telegram-bot. Configured development dependencies with pytest suite, coverage tools, basedpyright, and ruff. Enhanced type checking by setting basedpyright to strict mode and added pytest configuration requiring 100% test coverage. Verified configuration with zero type checking errors, warnings, or notes. Project configuration is complete and ready for directory structure creation.\n</info added on 2025-07-05T17:36:11.843Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Directory Structure Creation",
            "description": "Create the standard Python project directory structure including source, tests, and documentation folders",
            "dependencies": [
              1
            ],
            "details": "Create src/ directory for source code, tests/ for test files, docs/ for documentation, and any additional directories like scripts/ or examples/ as needed.\n<info added on 2025-07-05T17:21:34.670Z>\nCreate the directory structure according to the specifications in `docs/project_overview.md`. Follow the exact directory layout and organization defined in the project overview document to ensure consistency with the project architecture.\n</info added on 2025-07-05T17:21:34.670Z>\n<info added on 2025-07-05T17:44:51.651Z>\nSuccessfully implemented the complete directory structure as specified in the project overview document. Created all required directories and subdirectories including the main source structure (src/mover_status/ with app/, core/, config/, notifications/, plugins/, utils/ subdirectories), comprehensive test structure (tests/ with unit, integration, and fixture directories mirroring source layout), configuration directories (configs/examples/ and configs/schemas/), and all necessary Python package files (__init__.py files throughout). Generated core files including __main__.py, config.yaml, conftest.py, example configuration files (config_discord.yaml.example, config_telegram.yaml.example), JSON schema files (main_config_schema.json, provider_config_schema.json), and plugin template documentation. The implementation resulted in 32 Python files in source and 37 in tests, exactly matching the specification requirements. Directory structure is now ready for the next phase of __init__.py and exports configuration.\n</info added on 2025-07-05T17:44:51.651Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "__init__.py and Exports Configuration",
            "description": "Create __init__.py files and configure proper module exports for the package",
            "dependencies": [
              3
            ],
            "details": "Create __init__.py files in all package directories, define __all__ exports, set up version information, and configure main module imports.\n<info added on 2025-07-05T17:58:00.456Z>\nSuccessfully implemented __init__.py files and module exports configuration:\n\n1. Updated main package __init__.py with version info, metadata, and proper main() function\n2. Created comprehensive __init__.py files for all module packages:\n   - app/ - Application module with placeholder for Application class\n   - config/ - Configuration system with placeholders for ConfigManager, ConfigLoader, ConfigModels, ConfigValidator\n   - config/loader/ - YAML and environment variable loaders\n   - config/manager/ - Configuration lifecycle management\n   - config/models/ - Pydantic models for validation\n   - config/validator/ - Configuration validation\n   - core/ - Core functionality placeholders\n   - core/data/ - Data management\n   - core/data/filesystem/ - Directory scanning and size calculations\n   - core/monitor/ - Monitoring orchestration\n   - core/process/ - Process detection\n   - core/progress/ - Progress calculations\n   - notifications/ - Notification system\n   - notifications/base/ - Base provider classes\n   - notifications/manager/ - Provider management\n   - notifications/models/ - Message models\n   - plugins/ - Plugin system\n   - plugins/loader/ - Plugin loading\n   - plugins/discord/ - Discord provider\n   - plugins/discord/embeds/ - Discord embed generation\n   - plugins/discord/webhook/ - Discord webhook client\n   - plugins/telegram/ - Telegram provider\n   - plugins/telegram/bot/ - Telegram bot client\n   - plugins/telegram/formatting/ - Message formatting\n   - plugins/template/ - Plugin template\n   - utils/ - Utility functions\n   - utils/formatting/ - Progress/time/data formatting\n   - utils/logging/ - Structured logging\n   - utils/time/ - Time utilities\n   - utils/validation/ - Validation utilities\n3. Updated tests/__init__.py with proper module declaration\n4. All __init__.py files include proper docstrings, __future__ imports, and __all__ exports\n5. Used TODO comments for future class implementations\n6. Verified with basedpyright: 0 errors, 0 warnings, 0 notes\n\nThe module structure is now properly configured with clear export interfaces ready for implementation.\n</info added on 2025-07-05T17:58:00.456Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "LICENSE and README.md Creation",
            "description": "Create LICENSE file and comprehensive README.md with project documentation",
            "dependencies": [
              2
            ],
            "details": "Choose and create appropriate LICENSE file (MIT, Apache, etc.), write README.md with project description, installation instructions, usage examples, and contribution guidelines.\n<info added on 2025-07-05T18:06:16.695Z>\nSuccessfully completed LICENSE and README.md creation for the Python 3.13 implementation:\n\nLICENSE: Confirmed existing GNU AGPL v3 license is appropriate and remains unchanged. The copyleft license ensures any modifications to the codebase are also open source.\n\nREADME.md: Completely rewrote the README.md file to reflect the new Python 3.13 implementation:\n- Updated title to \"Mover Status Monitor\" \n- Replaced description to emphasize modern, modular Python architecture\n- Added comprehensive features section highlighting Python 3.13, plugin architecture, YAML config, type safety, etc.\n- Created detailed installation instructions with uv package manager\n- Added configuration examples for YAML files and environment variables\n- Included usage examples for both CLI and programmatic interfaces\n- Added development section with testing, type checking, and contribution guidelines\n- Maintained existing Telegram and Discord setup instructions as they remain relevant\n- Updated license section with proper file reference\n\nThe README now accurately represents the Python application architecture while maintaining the same core functionality and setup procedures for notification providers. All type checking passes with 0 errors, 0 warnings, 0 notes.\n</info added on 2025-07-05T18:06:16.695Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Test Scaffolding for Structure Validation",
            "description": "Set up initial test framework and create tests to validate the project structure and basic functionality",
            "dependencies": [
              4
            ],
            "details": "Create test_structure.py to validate directory structure, test_imports.py to verify module imports work correctly, configure pytest.ini, and set up basic CI/CD test workflows.\n<info added on 2025-07-05T18:11:51.996Z>\nSuccessfully implemented comprehensive test scaffolding for structure validation:\n\n1. Created tests/test_structure.py with comprehensive project structure validation:\n   - TestProjectStructure class with tests for directory structure validation\n   - TestPyprojectToml class with tests for project configuration validation\n   - TestCurrentPythonVersion class with tests for Python version requirements\n   - Tests verify all 65+ directories and files exist as specified\n   - Tests check that all __init__.py files exist in package directories\n   - Tests validate pyproject.toml configuration including dependencies, dev dependencies, pytest settings, and basedpyright configuration\n\n2. Created tests/test_imports.py with comprehensive module import validation:\n   - TestCoreImports class with tests for all core module imports\n   - TestPackageAttributes class with tests for package metadata and exports\n   - TestModuleStructure class with tests for circular import detection and module path validation\n   - TestTestsImports class with tests for test package imports\n   - Tests verify all 32+ source modules and 37+ test modules can be imported successfully\n   - Tests check for proper __all__ exports and docstrings\n\n3. Created tests/test_basic_functionality.py with basic functionality validation:\n   - TestBasicFunctionality class with tests for fixtures and configuration files\n   - TestProjectConfiguration class with tests for project consistency\n   - Tests verify pytest fixtures work correctly (temp_dir, sample_config)\n   - Tests validate configuration files exist and have content\n   - Tests check project name and Python version consistency\n\n4. Updated tests/conftest.py to properly import pytest and enable fixtures\n\n5. Installed all development dependencies including pytest, pytest-cov, pytest-asyncio, pytest-mock, basedpyright, and ruff\n\n6. Verified all tests pass: 40 tests passing with 0 failures\n7. Achieved 0 type checking errors, 0 warnings, 0 notes with basedpyright\n\nThe test scaffolding provides comprehensive validation of:\n- Directory structure matches specifications exactly\n- All Python packages can be imported without circular dependencies\n- Project configuration is consistent and correct\n- Test fixtures work properly for future test development\n- Python 3.13 compatibility is maintained\n- All required files exist and have proper content\n\nThis establishes a solid foundation for ongoing test development and ensures the project structure is robust and correctly configured.\n</info added on 2025-07-05T18:11:51.996Z>",
            "status": "done",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement Configuration System Foundation",
        "description": "Create the configuration loading system with YAML support, environment variable overrides, and Pydantic models for validation following TDD principles",
        "details": "Following TDD:\n1. Write tests/unit/config/models/test_base.py:\n```python\ndef test_base_config_model_validation():\n    # Test Pydantic model validation\n    pass\n\ndef test_config_merge_behavior():\n    # Test configuration merging logic\n    pass\n```\n\n2. Implement src/mover_status/config/models/base.py:\n```python\nfrom pydantic import BaseModel, Field\nfrom typing import Dict, Any\n\nclass BaseConfig(BaseModel):\n    class Config:\n        extra = \"forbid\"\n        validate_assignment = True\n```\n\n3. Write tests/unit/config/loader/test_yaml_loader.py\n4. Implement YAML loader with error handling:\n```python\nimport yaml\nfrom pathlib import Path\nfrom typing import Dict, Any\n\nclass YamlLoader:\n    def load(self, path: Path) -> Dict[str, Any]:\n        try:\n            with open(path, 'r') as f:\n                return yaml.safe_load(f) or {}\n        except Exception as e:\n            raise ConfigLoadError(f\"Failed to load {path}: {e}\")\n```\n\n5. Create environment variable override system\n6. Implement configuration merger with precedence rules",
        "testStrategy": "TDD approach:\n- Write failing tests for each configuration component\n- Test YAML parsing with valid/invalid files\n- Test environment variable override precedence\n- Test configuration validation with Pydantic\n- Test error handling for missing/malformed configs\n- Achieve 100% coverage on all configuration modules",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Design Pydantic Models for Configuration Schema",
            "description": "Create comprehensive Pydantic models that define the structure, types, and validation rules for all configuration parameters. Include nested models for complex configurations and custom validators for business logic constraints.",
            "dependencies": [],
            "details": "Define BaseModel classes with proper field types, default values, validation constraints, and custom validators. Include models for database settings, API configurations, logging parameters, and feature flags. Implement field descriptions and examples for auto-documentation.\n<info added on 2025-07-05T18:21:14.117Z>\nSuccessfully completed Pydantic models for configuration schema:\n\n**Implementation Summary:**\n1. **Base Configuration Models** (`base.py`):\n   - Created `BaseConfig` with strict Pydantic v2 settings (extra=\"forbid\", validate_assignment=True, etc.)\n   - Implemented `RetryConfig` with exponential backoff and timeout settings\n   - Implemented `RateLimitConfig` for notification rate limiting\n   - Created abstract `ConfigurableProvider` interface for plugin providers\n   - Added constant classes for `LogLevel`, `NotificationEvent`, and `ProviderName`\n\n2. **Monitoring Configuration Models** (`monitoring.py`):\n   - `MonitoringConfig`: Interval, detection timeout, and dry-run mode\n   - `ProcessConfig`: Process name and path patterns with validation\n   - `ProgressConfig`: Min change threshold, estimation window, and exclusion patterns\n   - `NotificationConfig`: Enabled providers, events, and rate limits with comprehensive validation\n   - `LoggingConfig`: Log level, format, and file path configuration\n\n3. **Provider Configuration Models** (`providers.py`):\n   - **Discord Provider**: Complete webhook configuration with embed colors, mentions, notifications, and retry settings\n   - **Telegram Provider**: Bot token, chat IDs, message formatting, templates, and notification settings\n   - **Validation**: Regex validation for webhook URLs and bot tokens, proper field constraints\n   - **ProviderConfig**: Container class for all provider configurations\n\n4. **Main Configuration Model** (`main.py`):\n   - `AppConfig`: Combines all configuration components\n   - Cross-component validation to ensure consistency between enabled providers and configurations\n   - Model validator to check that enabled providers are actually configured\n\n5. **Type Safety & Validation**:\n   - Used Python 3.13 modern typing (no deprecated Union/Optional/List/Dict imports)\n   - Comprehensive field validation with proper constraints\n   - Custom validators for complex business logic\n   - All models pass `uvx basedpyright` with 0 errors, 0 warnings, 0 notes\n\n**Key Features:**\n- **Comprehensive Validation**: Each model includes proper field constraints, custom validators, and error messages\n- **Modern Python 3.13**: Uses latest typing features and best practices\n- **Extensible Design**: Abstract base classes and interfaces support future providers\n- **Type Safety**: Strict type checking with basedpyright compliance\n- **Rich Descriptions**: All fields include helpful descriptions for auto-documentation\n- **Default Values**: Sensible defaults for all optional configuration parameters\n\nThe Pydantic models provide a solid foundation for the configuration system with strict validation, comprehensive type safety, and extensible architecture for future enhancements.\n</info added on 2025-07-05T18:21:14.117Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement YAML Configuration Loader",
            "description": "Build a robust YAML file loader that can parse configuration files, handle multiple file formats, and provide meaningful error messages for malformed YAML structures.",
            "dependencies": [
              1
            ],
            "details": "Create a ConfigLoader class that uses PyYAML to load configuration files. Implement support for multiple YAML files, include file validation, handle YAML parsing errors gracefully, and provide file path resolution for relative imports.\n<info added on 2025-07-05T18:43:30.817Z>\n**COMPLETED - YAML Configuration Loader Implementation**\n\nSuccessfully implemented a robust YAML configuration loader using Test-Driven Development approach with comprehensive error handling and type safety.\n\n**Key Implementation Details:**\n- **YamlLoader Class**: Core implementation using `yaml.safe_load()` for secure parsing with UTF-8 encoding support\n- **ConfigLoadError Exception**: Custom exception class with descriptive error messages and proper exception chaining\n- **Error Handling**: Graceful handling of missing files, permission errors, malformed YAML, empty files, and null content\n- **Type Safety**: Full compliance with basedpyright strict type checking, returns `dict[str, object]` for maximum type safety\n- **Security**: Uses safe YAML loading to prevent code execution vulnerabilities\n\n**Testing & Quality Assurance:**\n- Comprehensive test suite with 9 test cases covering all scenarios including edge cases and error conditions\n- 100% test coverage for the YAML loader module\n- All tests pass successfully with pytest best practices\n- Zero errors, warnings, or notes from uvx basedpyright type checking\n\n**Integration Ready:**\n- Updated module exports in `__init__.py` for `YamlLoader` and `ConfigLoadError`\n- Supports complex nested YAML structures and various data types\n- Foundation established for environment variable override system integration\n\nThe implementation provides a solid, secure, and well-tested foundation for the configuration system's file loading capabilities.\n</info added on 2025-07-05T18:43:30.817Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build Environment Variable Override System",
            "description": "Develop a system that can automatically map environment variables to configuration fields using naming conventions and explicit mappings, with proper type conversion and validation.",
            "dependencies": [
              1
            ],
            "details": "Implement environment variable parsing with configurable prefixes, nested field mapping using dot notation or underscores, automatic type conversion for primitive types, and support for complex types through JSON parsing of env vars.\n<info added on 2025-07-05T18:53:06.623Z>\nSuccessfully completed Environment Variable Override System implementation following TDD principles.\n\nImplementation Summary:\n\nCore Components:\n1. EnvLoader Class: Main implementation with support for configurable prefixes (default: \"MOVER_STATUS_\"), nested structure mapping via underscores or dots, optional automatic type conversion (bool, int, float, JSON), custom environment variable mappings, and graceful error handling.\n\n2. EnvLoadError Exception: Custom exception with detailed error messages and environment variable context.\n\nKey Features:\n- Flexible Naming: Supports both underscore and dot separators for nested structures\n- Type Conversion: Automatic conversion of strings to appropriate Python types (bool, int, float, lists, dicts via JSON)\n- Custom Mappings: Ability to map specific environment variables to config paths\n- Precedence: Custom mappings take precedence over prefix-based loading\n- Error Handling: Comprehensive error handling with meaningful messages\n- Type Safety: Full compliance with basedpyright strict type checking\n\nTesting & Quality:\n- TDD Approach: Implemented following Test-Driven Development with tests written first\n- 100% Test Coverage: Comprehensive test suite with 21 test cases covering all functionality\n- Zero Type Issues: All code passes uvx basedpyright with 0 errors, 0 warnings, 0 notes\n- Edge Cases: Tests cover empty values, type conversion failures, custom mappings, nested overrides\n\nIntegration:\n- Updated module exports in __init__.py for EnvLoader and EnvLoadError\n- Ready for integration with configuration merging system\n- Foundation established for next subtask (Configuration Merging Logic)\n\nThe Environment Variable Override System provides a robust, type-safe, and well-tested foundation for loading configuration from environment variables with sophisticated mapping and conversion capabilities.\n</info added on 2025-07-05T18:53:06.623Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Configuration Merging Logic",
            "description": "Create a sophisticated merging system that combines configurations from multiple sources (defaults, files, environment variables) with proper precedence rules and conflict resolution.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Build a ConfigMerger that handles deep merging of nested dictionaries, implements precedence order (env vars > config files > defaults), resolves conflicts intelligently, and maintains audit trail of configuration sources.\n<info added on 2025-07-05T19:04:44.072Z>\nSuccessfully completed Configuration Merging Logic implementation following Test-Driven Development principles.\n\nImplementation Summary:\n\nCore Components:\n1. ConfigMerger Class: Main implementation with sophisticated merging capabilities including deep merging of nested dictionaries with proper precedence rules, support for multiple configuration sources with left-to-right precedence (later sources override earlier ones), audit trail tracking for configuration source attribution, runtime type validation for dictionary inputs, and preservation of original configuration objects through deep copying.\n\n2. ConfigMergeError Exception: Custom exception with detailed error messages and optional configuration path context for debugging.\n\nKey Features:\n- Deep Merging: Recursively merges nested dictionaries while preserving structure\n- Type Conflict Resolution: Handles type conflicts by replacing values (lists and primitives are completely replaced, not merged)\n- Audit Trail: Optional source tracking showing which configuration source provided each value\n- Precedence Rules: Implements standard configuration precedence (environment variables > config files > defaults)\n- Error Handling: Comprehensive validation and error reporting with meaningful messages\n- Memory Safety: Deep copying ensures original configurations are never modified\n\nTesting & Quality:\n- TDD Approach: Implemented following Test-Driven Development with tests written first\n- 100% Test Coverage: Comprehensive test suite with 17 test cases covering all functionality\n- Type Safety: Complies with basedpyright strict type checking (0 errors, warnings are acceptable for flexible config system)\n- Edge Cases: Tests cover empty configs, type conflicts, audit trail management, error conditions\n\nIntegration:\n- Updated module exports in __init__.py for ConfigMerger and ConfigMergeError\n- Ready for integration with YAML loader, environment variable loader, and Pydantic models\n- Provides foundation for next subtask (Comprehensive Error Handling)\n\nUsage Example:\nmerger = ConfigMerger(track_sources=True)\ndefaults = {\"timeout\": 30, \"retries\": 3}\nfile_config = {\"timeout\": 60, \"host\": \"localhost\"}\nenv_config = {\"timeout\": 90}\n\nresult = merger.merge_multiple([defaults, file_config, env_config])\n# Result: {\"timeout\": 90, \"retries\": 3, \"host\": \"localhost\"}\n\naudit_trail = merger.get_audit_trail()\n# Shows which source provided each configuration value\n\nThe Configuration Merging Logic provides a robust, well-tested foundation for combining configuration from multiple sources with sophisticated precedence rules and comprehensive error handling.\n</info added on 2025-07-05T19:04:44.072Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Develop Comprehensive Error Handling",
            "description": "Implement robust error handling throughout the configuration system with custom exceptions, detailed error messages, and graceful degradation strategies.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create custom exception classes for different error types (validation, parsing, merging), implement detailed error reporting with field paths and suggestions, add logging for configuration issues, and provide fallback mechanisms for non-critical configuration errors.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Write TDD Test Suite for All Components",
            "description": "Develop comprehensive test coverage for all configuration system components using Test-Driven Development principles, including unit tests, integration tests, and edge case scenarios.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Write pytest test cases for Pydantic model validation, YAML loading edge cases, environment variable parsing, configuration merging scenarios, error handling paths, and end-to-end configuration loading. Include fixtures for test data and mock environments.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Create Configuration System Documentation",
            "description": "Write comprehensive documentation covering configuration schema, usage patterns, precedence rules, and troubleshooting guides for the entire configuration system.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Create user guides for configuration file structure, environment variable naming conventions, merging behavior examples, API reference documentation, troubleshooting common issues, and best practices for configuration management in different deployment scenarios.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Build Logging Infrastructure",
        "description": "Implement structured logging system with configurable handlers, formatters, and verbosity levels to support debugging and production monitoring",
        "details": "TDD Implementation:\n1. Write tests/unit/utils/logging/test_logger.py:\n```python\ndef test_logger_initialization():\n    logger = Logger(\"test\")\n    assert logger.name == \"test\"\n\ndef test_structured_logging():\n    # Test JSON output format\n    pass\n```\n\n2. Implement src/mover_status/utils/logging/logger.py:\n```python\nimport logging\nimport json\nfrom typing import Any, Dict\n\nclass StructuredFormatter(logging.Formatter):\n    def format(self, record: logging.LogRecord) -> str:\n        log_obj = {\n            \"timestamp\": self.formatTime(record),\n            \"level\": record.levelname,\n            \"logger\": record.name,\n            \"message\": record.getMessage(),\n            \"extra\": getattr(record, \"extra\", {})\n        }\n        return json.dumps(log_obj)\n```\n\n3. Create configurable handlers (console, file, syslog)\n4. Implement log level configuration from config/CLI\n5. Add context managers for temporary log level changes\n6. Create correlation ID tracking for request tracing",
        "testStrategy": "Test coverage requirements:\n- Test all log levels and formatting options\n- Test handler configuration and rotation\n- Test structured logging with extra fields\n- Test thread-safe logging operations\n- Mock file I/O for handler tests\n- Verify JSON output structure",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Structured Formatter",
            "description": "Create a structured logging formatter that supports JSON, key-value pairs, and custom field formatting with configurable timestamp formats and field ordering.",
            "dependencies": [],
            "details": "Develop a flexible formatter class that can output logs in structured formats (JSON, logfmt, etc.), handle custom fields, support different timestamp formats, and allow field customization. Include support for nested objects and arrays in log messages.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Setup Multiple Log Handlers",
            "description": "Implement console, file, and syslog handlers with configurable output destinations, rotation policies, and format-specific configurations.",
            "dependencies": [
              1
            ],
            "details": "Create handler classes for console output (with color support), file logging (with rotation by size/time), and syslog integration. Each handler should support the structured formatter and have independent configuration options for filtering and formatting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure Log Level Management",
            "description": "Implement dynamic log level configuration with support for per-module/logger level settings and runtime level changes.",
            "dependencies": [],
            "details": "Create a log level management system that supports standard levels (DEBUG, INFO, WARN, ERROR, FATAL), allows per-logger level configuration, supports runtime level changes, and includes level filtering at both logger and handler levels.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Develop Context Manager for Log Levels",
            "description": "Create thread-safe context managers for temporarily changing log levels and adding contextual information to log messages.",
            "dependencies": [
              3
            ],
            "details": "Implement context managers that can temporarily override log levels, add contextual fields to all log messages within the context, and ensure thread safety. Support nested contexts and proper cleanup on context exit.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Correlation ID Tracking",
            "description": "Build a correlation ID system that automatically tracks and includes correlation IDs in log messages across thread boundaries and async contexts.",
            "dependencies": [
              1,
              4
            ],
            "details": "Create a correlation ID tracking system using thread-local storage and context variables for async operations. Automatically generate UUIDs, propagate IDs across function calls, and include them in all log messages. Support manual ID setting and retrieval.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Develop Comprehensive TDD Test Coverage",
            "description": "Create extensive test suite covering all logging components, edge cases, thread safety, performance, and integration scenarios using TDD methodology.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Write comprehensive unit tests for all components including formatter behavior, handler functionality, log level management, context managers, and correlation ID tracking. Include integration tests, thread safety tests, performance benchmarks, and edge case handling. Achieve >95% code coverage.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Create Process Detection Framework",
        "description": "Build the abstract process detection interface and Unraid-specific implementation using psutil and proc filesystem for reliable mover process monitoring",
        "details": "TDD Development:\n1. Write tests/unit/core/process/test_detector.py:\n```python\nfrom abc import ABC, abstractmethod\n\ndef test_detector_interface():\n    # Test abstract interface contract\n    pass\n```\n\n2. Create abstract detector interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Optional, List\nfrom .models import ProcessInfo\n\nclass ProcessDetector(ABC):\n    @abstractmethod\n    def detect_mover(self) -> Optional[ProcessInfo]:\n        \"\"\"Detect running mover process\"\"\"\n        pass\n    \n    @abstractmethod\n    def is_process_running(self, pid: int) -> bool:\n        \"\"\"Check if process is still running\"\"\"\n        pass\n```\n\n3. Write tests/unit/core/process/test_unraid_detector.py\n4. Implement Unraid detector:\n```python\nimport psutil\nfrom typing import Optional\n\nclass UnraidMoverDetector(ProcessDetector):\n    MOVER_PATTERNS = [\"mover\", \"/usr/local/sbin/mover\"]\n    \n    def detect_mover(self) -> Optional[ProcessInfo]:\n        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):\n            try:\n                cmdline = ' '.join(proc.info['cmdline'] or [])\n                if any(pattern in cmdline for pattern in self.MOVER_PATTERNS):\n                    return ProcessInfo(\n                        pid=proc.info['pid'],\n                        command=cmdline,\n                        start_time=proc.create_time()\n                    )\n            except (psutil.NoSuchProcess, psutil.AccessDenied):\n                continue\n        return None\n```",
        "testStrategy": "Comprehensive testing:\n- Mock psutil process iteration\n- Test pattern matching for various mover commands\n- Test error handling for permission denied\n- Test process lifecycle (start, running, stopped)\n- Test cross-platform compatibility\n- Use fixtures for consistent test data",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Abstract Interface Definition",
            "description": "Design and implement abstract interfaces for process detection functionality, defining contracts for process discovery, filtering, and monitoring operations.",
            "dependencies": [],
            "details": "Create abstract base classes and interfaces that define the contract for process detection operations. Include methods for process enumeration, filtering by criteria, status checking, and event handling. Define data structures for process information representation and establish extensibility points for platform-specific implementations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Unraid-Specific Implementation",
            "description": "Develop concrete implementation of the abstract interface tailored for Unraid system architecture and process management specifics.",
            "dependencies": [
              1
            ],
            "details": "Implement the abstract interface for Unraid environment, utilizing system-specific APIs and file system structures. Handle Unraid's unique process hierarchy, container management, and system service detection. Integrate with Unraid's logging and monitoring systems for comprehensive process tracking.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Process Pattern Matching Logic",
            "description": "Implement sophisticated pattern matching algorithms for process identification, filtering, and categorization based on various criteria.",
            "dependencies": [
              1
            ],
            "details": "Develop flexible pattern matching system supporting regex, wildcard, and custom matching rules for process names, arguments, and attributes. Implement process grouping, hierarchy detection, and relationship mapping. Create configurable filtering mechanisms for different use cases and performance optimization.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Error and Permission Handling",
            "description": "Implement comprehensive error handling and permission management for secure and reliable process detection operations.",
            "dependencies": [
              2,
              3
            ],
            "details": "Design robust error handling for permission denied scenarios, system resource limitations, and process state changes. Implement graceful degradation when certain processes are inaccessible. Create logging and monitoring for error conditions, and establish retry mechanisms for transient failures.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "TDD Test Suite for Interface and Implementation",
            "description": "Develop comprehensive test-driven development suite covering both abstract interfaces and concrete implementations with mock scenarios.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create unit tests for abstract interface contracts, integration tests for Unraid-specific implementation, and mock frameworks for testing without system dependencies. Include performance benchmarks, edge case testing, and validation of error handling scenarios. Implement continuous testing pipeline for reliability assurance.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Cross-Platform Considerations",
            "description": "Analyze and implement cross-platform compatibility strategies and extension points for future platform support beyond Unraid.",
            "dependencies": [
              1,
              2,
              5
            ],
            "details": "Evaluate architectural decisions for cross-platform extensibility, identify common patterns across different operating systems, and create framework for adding new platform implementations. Document platform-specific requirements and establish guidelines for future platform integrations while maintaining code reusability.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement Filesystem Operations",
        "description": "Create efficient directory scanning and size calculation modules with configurable exclusion patterns for accurate progress tracking",
        "details": "TDD Implementation:\n1. Write tests/unit/core/data/filesystem/test_scanner.py:\n```python\ndef test_directory_scanning():\n    # Test recursive directory traversal\n    pass\n\ndef test_exclusion_patterns():\n    # Test pattern matching for exclusions\n    pass\n```\n\n2. Implement filesystem scanner:\n```python\nfrom pathlib import Path\nfrom typing import Iterator, Set\nimport fnmatch\n\nclass FilesystemScanner:\n    def __init__(self, exclusions: Set[str] = None):\n        self.exclusions = exclusions or {\".snapshots\", \".Recycle.Bin\", \"@eaDir\"}\n    \n    def scan_directory(self, path: Path) -> Iterator[Path]:\n        \"\"\"Yield all files in directory tree, respecting exclusions\"\"\"\n        try:\n            for item in path.iterdir():\n                if self._should_exclude(item):\n                    continue\n                \n                if item.is_file():\n                    yield item\n                elif item.is_dir():\n                    yield from self.scan_directory(item)\n        except PermissionError:\n            # Log and continue\n            pass\n    \n    def _should_exclude(self, path: Path) -> bool:\n        return any(fnmatch.fnmatch(path.name, pattern) \n                  for pattern in self.exclusions)\n```\n\n3. Create size calculator with caching:\n```python\nclass SizeCalculator:\n    def __init__(self, scanner: FilesystemScanner):\n        self.scanner = scanner\n        self._cache: Dict[Path, int] = {}\n    \n    def calculate_size(self, path: Path) -> int:\n        if path in self._cache:\n            return self._cache[path]\n        \n        total = sum(f.stat().st_size for f in self.scanner.scan_directory(path))\n        self._cache[path] = total\n        return total\n```",
        "testStrategy": "Test requirements:\n- Use temp directories with known file structures\n- Test exclusion pattern matching\n- Test permission error handling\n- Test symlink handling\n- Test large directory performance\n- Mock filesystem for edge cases\n- Verify cache behavior",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Directory Scanning Logic",
            "description": "Develop core directory traversal functionality with recursive scanning capabilities, including depth control and efficient file system navigation patterns.",
            "dependencies": [],
            "details": "Create a robust directory scanner that can traverse file systems recursively, handle different directory structures, implement depth limiting, and provide configurable scanning strategies (breadth-first vs depth-first). Include proper resource management and memory-efficient iteration.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Build Exclusion Pattern System",
            "description": "Design and implement a flexible pattern matching system for excluding files and directories based on various criteria like glob patterns, regex, and custom rules.",
            "dependencies": [
              1
            ],
            "details": "Create a comprehensive exclusion system supporting glob patterns, regular expressions, file extensions, directory names, and custom filtering rules. Implement pattern compilation for performance, support for .gitignore-style patterns, and configurable exclusion precedence.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Develop Size Calculation with Caching",
            "description": "Implement efficient file and directory size calculation with intelligent caching mechanisms to avoid redundant computations and improve performance.",
            "dependencies": [
              1,
              2
            ],
            "details": "Build a size calculation engine with multi-level caching (file-level, directory-level), cache invalidation strategies based on modification times, memory-efficient cache storage, and support for different size calculation modes (apparent size vs disk usage).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Handle Symlinks and Permission Errors",
            "description": "Implement robust error handling for symbolic links, permission denied scenarios, and other file system access issues with appropriate fallback strategies.",
            "dependencies": [
              1
            ],
            "details": "Create comprehensive error handling for symlink loops, broken symlinks, permission denied errors, network drive timeouts, and other file system exceptions. Implement configurable behavior for symlink following, graceful degradation, and detailed error reporting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Develop TDD Test Suite for All Scenarios",
            "description": "Create comprehensive test-driven development suite covering all functionality including edge cases, error conditions, and performance scenarios.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Build extensive test coverage including unit tests for each component, integration tests for complete workflows, performance benchmarks, edge case testing (empty directories, large files, deep nesting), mock file systems for controlled testing, and automated test execution pipeline.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "Build Progress Calculation Engine",
        "description": "Develop the core progress calculation algorithms including percentage completion, transfer rate calculation, and intelligent ETC estimation",
        "details": "TDD Development:\n1. Write tests/unit/core/progress/test_calculator.py:\n```python\ndef test_progress_percentage_calculation():\n    calc = ProgressCalculator()\n    result = calc.calculate_progress(transferred=50, total=100)\n    assert result.percentage == 50.0\n\ndef test_zero_size_handling():\n    # Test edge case of zero total size\n    pass\n```\n\n2. Implement progress calculator:\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport time\n\n@dataclass\nclass ProgressMetrics:\n    percentage: float\n    bytes_remaining: int\n    transfer_rate: float\n    etc_seconds: Optional[int]\n\nclass ProgressCalculator:\n    def __init__(self):\n        self.history: List[Tuple[float, int]] = []  # (timestamp, bytes)\n    \n    def calculate_progress(self, transferred: int, total: int) -> ProgressMetrics:\n        if total == 0:\n            return ProgressMetrics(100.0, 0, 0.0, 0)\n        \n        percentage = (transferred / total) * 100\n        remaining = total - transferred\n        \n        # Calculate transfer rate using moving average\n        now = time.time()\n        self.history.append((now, transferred))\n        \n        # Keep last 10 samples for rate calculation\n        self.history = self.history[-10:]\n        \n        rate = self._calculate_transfer_rate()\n        etc = self._estimate_completion_time(remaining, rate)\n        \n        return ProgressMetrics(percentage, remaining, rate, etc)\n    \n    def _calculate_transfer_rate(self) -> float:\n        if len(self.history) < 2:\n            return 0.0\n        \n        time_delta = self.history[-1][0] - self.history[0][0]\n        bytes_delta = self.history[-1][1] - self.history[0][1]\n        \n        if time_delta > 0:\n            return bytes_delta / time_delta\n        return 0.0\n```",
        "testStrategy": "Test scenarios:\n- Various progress percentages\n- Zero and negative values\n- Transfer rate with different histories\n- ETC calculation accuracy\n- Moving average behavior\n- Edge cases (stalled transfers, bursts)\n- Performance with large datasets",
        "priority": "high",
        "dependencies": [
          4,
          5
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Progress Percentage Calculation",
            "description": "Implement accurate progress percentage calculation logic that handles various data types, edge cases, and provides real-time updates for different progress tracking scenarios.",
            "dependencies": [],
            "details": "Create functions to calculate progress percentages from different input types (bytes transferred, items processed, time elapsed). Handle edge cases like zero denominators, negative values, and overflow conditions. Implement percentage capping at 100% and provide configurable precision levels.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Transfer Rate Computation",
            "description": "Develop transfer rate computation algorithms that calculate instantaneous and average transfer rates with proper unit handling and smoothing mechanisms.",
            "dependencies": [
              1
            ],
            "details": "Implement rate calculation for bytes/second, items/second, and other units. Create smoothing algorithms to handle rate fluctuations. Add support for different time windows and rate averaging methods. Include proper unit conversion and formatting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "ETC Estimation Logic",
            "description": "Create sophisticated Estimated Time to Completion (ETC) algorithms that use multiple prediction methods and adapt to changing transfer patterns.",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement linear projection, exponential smoothing, and adaptive ETC algorithms. Handle scenarios with variable transfer rates, paused operations, and network fluctuations. Provide confidence intervals and multiple ETC estimates based on different prediction models.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Moving Average and History Management",
            "description": "Implement robust historical data management system with configurable moving averages, data retention policies, and efficient storage mechanisms.",
            "dependencies": [
              2
            ],
            "details": "Create circular buffers for efficient historical data storage. Implement various moving average algorithms (simple, weighted, exponential). Add configurable retention policies and memory management. Include data compression and sampling strategies for long-running operations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Comprehensive TDD Test Suite",
            "description": "Develop a complete test-driven development suite covering all progress tracking components with extensive edge case testing and performance validation.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Create unit tests for all calculation functions, integration tests for component interactions, and performance tests for large datasets. Include edge case testing for network interruptions, zero/negative values, and boundary conditions. Add mock data generators and automated test scenarios for various progress tracking patterns.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "Develop CLI Interface",
        "description": "Create the command-line interface using Click with argument parsing, dry-run mode, help system, and configuration file support",
        "details": "TDD Implementation:\n1. Write tests/unit/app/test_cli.py:\n```python\nfrom click.testing import CliRunner\n\ndef test_cli_basic_invocation():\n    runner = CliRunner()\n    result = runner.invoke(cli, ['--help'])\n    assert result.exit_code == 0\n    assert 'Mover Status Monitor' in result.output\n```\n\n2. Implement CLI with Click:\n```python\nimport click\nfrom pathlib import Path\nfrom typing import Optional\n\n@click.command()\n@click.option('--config', '-c', \n              type=click.Path(exists=True, path_type=Path),\n              default='config.yaml',\n              help='Configuration file path')\n@click.option('--dry-run', '-d',\n              is_flag=True,\n              help='Run without sending notifications')\n@click.option('--log-level', '-l',\n              type=click.Choice(['DEBUG', 'INFO', 'WARNING', 'ERROR']),\n              default='INFO',\n              help='Logging verbosity')\n@click.option('--once', '-o',\n              is_flag=True,\n              help='Run once and exit')\n@click.version_option()\ndef cli(config: Path, dry_run: bool, log_level: str, once: bool) -> None:\n    \"\"\"Mover Status Monitor - Track Unraid mover progress\"\"\"\n    from .runner import ApplicationRunner\n    \n    runner = ApplicationRunner(\n        config_path=config,\n        dry_run=dry_run,\n        log_level=log_level,\n        run_once=once\n    )\n    \n    try:\n        runner.run()\n    except KeyboardInterrupt:\n        click.echo(\"\\nShutting down gracefully...\")\n    except Exception as e:\n        click.echo(f\"Error: {e}\", err=True)\n        raise click.ClickException(str(e))\n```\n\n3. Add validation for mutually exclusive options\n4. Implement configuration file discovery\n5. Add shell completion support",
        "testStrategy": "CLI testing strategy:\n- Test all command-line options\n- Test option combinations\n- Test error handling and messages\n- Test configuration file loading\n- Test dry-run mode behavior\n- Use CliRunner for isolated testing\n- Verify exit codes",
        "priority": "medium",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "CLI Command and Option Definitions",
            "description": "Define the main CLI commands, subcommands, and their associated options using Click decorators. Implement command structure with proper grouping and option types.",
            "dependencies": [],
            "details": "Create the main CLI entry point with Click groups and commands. Define all command-line options with appropriate types (string, int, bool, choice), help text, and default values. Implement command hierarchy and ensure proper option inheritance where needed.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Argument Parsing and Validation",
            "description": "Implement robust argument parsing with comprehensive validation logic for all CLI inputs and edge cases.",
            "dependencies": [
              1
            ],
            "details": "Add custom validation functions for complex argument types. Implement input sanitization and error handling for invalid arguments. Create validation for file paths, URLs, numeric ranges, and other domain-specific inputs. Ensure proper error messages for validation failures.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configuration File Support",
            "description": "Implement configuration file discovery, loading, and merging with command-line arguments, supporting multiple formats and locations.",
            "dependencies": [
              1,
              2
            ],
            "details": "Add support for YAML, JSON, and TOML configuration files. Implement config file discovery in standard locations (user home, project root, etc.). Create configuration precedence logic (CLI args > env vars > config file > defaults). Add config validation and schema checking.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Shell Completion and Help System",
            "description": "Implement comprehensive shell completion for bash, zsh, and fish, along with an enhanced help system with examples and usage patterns.",
            "dependencies": [
              1,
              2
            ],
            "details": "Generate shell completion scripts for all major shells. Implement dynamic completion for file paths, available options, and context-aware suggestions. Create detailed help text with usage examples, command descriptions, and option explanations. Add man page generation capability.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "TDD Test Coverage for CLI Behaviors",
            "description": "Develop comprehensive test suite covering all CLI functionality including edge cases, error conditions, and integration scenarios.",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Write unit tests for all CLI commands and options using Click's testing utilities. Test argument validation, configuration file loading, error handling, and output formatting. Create integration tests for complete CLI workflows. Implement test fixtures for various configuration scenarios and mock external dependencies.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Create Notification Provider Architecture",
        "description": "Build the abstract notification provider base classes, registry system, and message dispatch infrastructure with retry logic and timeout handling",
        "details": "TDD Development:\n1. Write tests/unit/notifications/base/test_provider.py:\n```python\ndef test_provider_interface():\n    # Test abstract provider contract\n    pass\n\ndef test_retry_decorator():\n    # Test exponential backoff retry logic\n    pass\n```\n\n2. Implement base provider:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, Optional\nimport asyncio\nfrom functools import wraps\n\nclass NotificationProvider(ABC):\n    def __init__(self, config: Dict[str, Any]):\n        self.config = config\n        self.enabled = config.get('enabled', True)\n    \n    @abstractmethod\n    async def send_notification(self, message: Message) -> bool:\n        \"\"\"Send notification, return success status\"\"\"\n        pass\n    \n    @abstractmethod\n    def validate_config(self) -> None:\n        \"\"\"Validate provider configuration\"\"\"\n        pass\n\ndef with_retry(max_attempts: int = 3, backoff_factor: float = 2.0):\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            for attempt in range(max_attempts):\n                try:\n                    return await func(*args, **kwargs)\n                except Exception as e:\n                    if attempt == max_attempts - 1:\n                        raise\n                    wait_time = backoff_factor ** attempt\n                    await asyncio.sleep(wait_time)\n            return None\n        return wrapper\n    return decorator\n```\n\n3. Create provider registry:\n```python\nclass ProviderRegistry:\n    def __init__(self):\n        self._providers: Dict[str, Type[NotificationProvider]] = {}\n    \n    def register(self, name: str, provider_class: Type[NotificationProvider]):\n        self._providers[name] = provider_class\n    \n    def create_provider(self, name: str, config: Dict[str, Any]) -> NotificationProvider:\n        if name not in self._providers:\n            raise ValueError(f\"Unknown provider: {name}\")\n        return self._providers[name](config)\n```",
        "testStrategy": "Testing approach:\n- Mock async operations\n- Test retry logic with failures\n- Test timeout handling\n- Test provider registration\n- Test configuration validation\n- Test parallel dispatch\n- Verify error propagation",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Abstract Provider Base Class",
            "description": "Design and implement an abstract base class that defines the interface for all notification providers, including methods for sending notifications, handling authentication, and provider-specific configuration.",
            "dependencies": [],
            "details": "Define abstract methods for send(), validate_config(), get_provider_name(), and handle_response(). Include common properties like provider_id, config, and status. Establish the contract that all concrete providers must implement.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Retry and Timeout Logic",
            "description": "Build a robust retry mechanism with exponential backoff, circuit breaker pattern, and configurable timeout handling for failed notification attempts.",
            "dependencies": [
              1
            ],
            "details": "Create retry decorator with configurable max attempts, backoff strategy, and timeout values. Implement circuit breaker to prevent cascading failures. Add logging for retry attempts and failure tracking.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Develop Provider Registry System",
            "description": "Create a registry system that dynamically discovers, registers, and manages notification providers with support for plugin-style architecture.",
            "dependencies": [
              1
            ],
            "details": "Implement provider discovery mechanism, registration/deregistration methods, provider lifecycle management, and plugin loading capabilities. Include provider metadata storage and retrieval functions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build Configuration Validation System",
            "description": "Implement comprehensive configuration validation for providers, including schema validation, credential verification, and environment-specific settings.",
            "dependencies": [
              1,
              3
            ],
            "details": "Create validation schemas for each provider type, implement credential testing mechanisms, add environment variable support, and provide clear error messages for configuration issues.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create Async Dispatch Infrastructure",
            "description": "Build the core asynchronous notification dispatch system with queue management, load balancing, and concurrent processing capabilities.",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Implement async task queue, worker pool management, message routing logic, and delivery status tracking. Include priority handling and batch processing capabilities for high-volume scenarios.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Develop TDD Test Suite",
            "description": "Create comprehensive test-driven development suite covering all components with unit tests, integration tests, and mock provider implementations.",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Write unit tests for each component, create mock providers for testing, implement integration tests for the complete notification flow, add performance tests, and establish test coverage requirements.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement Discord Provider",
        "description": "Create the Discord notification provider with webhook integration, rich embed generation, progress visualization, and error handling",
        "details": "TDD Implementation:\n1. Write tests/unit/plugins/discord/test_provider.py:\n```python\nimport pytest\nfrom unittest.mock import AsyncMock, patch\n\n@pytest.mark.asyncio\nasync def test_discord_webhook_send():\n    provider = DiscordProvider({\"webhook_url\": \"https://discord.com/api/webhooks/...\"}) \n    with patch('httpx.AsyncClient.post') as mock_post:\n        mock_post.return_value.status_code = 204\n        result = await provider.send_notification(test_message)\n        assert result is True\n```\n\n2. Implement Discord provider:\n```python\nimport httpx\nfrom typing import Dict, Any\nimport asyncio\n\nclass DiscordProvider(NotificationProvider):\n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self.webhook_url = config['webhook_url']\n        self.username = config.get('username', 'Mover Status')\n        self.avatar_url = config.get('avatar_url')\n    \n    @with_retry(max_attempts=3)\n    async def send_notification(self, message: Message) -> bool:\n        embed = self._build_embed(message)\n        \n        payload = {\n            \"username\": self.username,\n            \"avatar_url\": self.avatar_url,\n            \"embeds\": [embed]\n        }\n        \n        async with httpx.AsyncClient(timeout=30.0) as client:\n            response = await client.post(self.webhook_url, json=payload)\n            response.raise_for_status()\n            return True\n    \n    def _build_embed(self, message: Message) -> Dict[str, Any]:\n        color = self._get_color_for_status(message.status)\n        \n        embed = {\n            \"title\": message.title,\n            \"description\": message.description,\n            \"color\": color,\n            \"fields\": [],\n            \"timestamp\": message.timestamp.isoformat()\n        }\n        \n        if message.progress:\n            embed[\"fields\"].append({\n                \"name\": \"Progress\",\n                \"value\": f\"{message.progress.percentage:.1f}%\",\n                \"inline\": True\n            })\n            \n            if message.progress.etc_seconds:\n                embed[\"fields\"].append({\n                    \"name\": \"ETC\",\n                    \"value\": self._format_time(message.progress.etc_seconds),\n                    \"inline\": True\n                })\n        \n        return embed\n```",
        "testStrategy": "Discord provider tests:\n- Mock HTTP requests\n- Test embed generation\n- Test color mapping\n- Test rate limit handling\n- Test webhook validation\n- Test error responses\n- Verify payload structure",
        "priority": "medium",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Discord Webhook Integration",
            "description": "Set up Discord webhook client with authentication, connection management, and message sending capabilities",
            "dependencies": [],
            "details": "Create webhook client class with methods for sending messages, handling authentication tokens, managing connection pooling, and implementing retry logic for failed webhook deliveries",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Develop Rich Embed Generation Logic",
            "description": "Build system to generate Discord-compatible rich embeds with dynamic content formatting",
            "dependencies": [
              1
            ],
            "details": "Create embed builder with support for titles, descriptions, fields, colors, thumbnails, timestamps, and footer information. Include templates for different notification types and dynamic content injection",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create Progress Visualization Components",
            "description": "Implement visual progress indicators and status displays for Discord embeds",
            "dependencies": [
              2
            ],
            "details": "Design progress bars, status badges, completion percentages, and timeline visualizations that can be embedded in Discord messages. Include real-time updates and milestone tracking",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Error and Rate Limit Handling",
            "description": "Build robust error handling and rate limiting system for Discord API interactions",
            "dependencies": [
              1
            ],
            "details": "Implement exponential backoff for rate limits, error classification and recovery strategies, webhook validation, fallback mechanisms, and comprehensive logging for debugging failed deliveries",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Develop Comprehensive TDD Test Coverage",
            "description": "Create complete test suite covering all Discord integration features with test-driven development approach",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Write unit tests for webhook client, embed generation, progress visualization, error handling, and integration tests for end-to-end workflows. Include mock Discord API responses and edge case scenarios",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 10,
        "title": "Build Monitoring Orchestrator",
        "description": "Create the main monitoring orchestration system with state machine, event bus, and coordination of all components for the complete monitoring lifecycle",
        "details": "TDD Implementation:\n1. Write tests/unit/core/monitor/test_orchestrator.py:\n```python\ndef test_orchestrator_lifecycle():\n    orchestrator = MonitorOrchestrator()\n    # Test state transitions\n    pass\n\ndef test_event_handling():\n    # Test event bus integration\n    pass\n```\n\n2. Implement orchestrator:\n```python\nfrom enum import Enum, auto\nfrom typing import Optional\nimport asyncio\n\nclass MonitorState(Enum):\n    IDLE = auto()\n    DETECTING = auto()\n    MONITORING = auto()\n    COMPLETING = auto()\n    ERROR = auto()\n\nclass MonitorOrchestrator:\n    def __init__(self, \n                 detector: ProcessDetector,\n                 calculator: ProgressCalculator,\n                 notifier: NotificationManager,\n                 config: Config):\n        self.detector = detector\n        self.calculator = calculator\n        self.notifier = notifier\n        self.config = config\n        self.state = MonitorState.IDLE\n        self.current_process: Optional[ProcessInfo] = None\n        self._running = False\n    \n    async def run(self) -> None:\n        self._running = True\n        \n        while self._running:\n            try:\n                await self._run_cycle()\n                await asyncio.sleep(self.config.check_interval)\n            except Exception as e:\n                await self._handle_error(e)\n    \n    async def _run_cycle(self) -> None:\n        if self.state == MonitorState.IDLE:\n            await self._detect_process()\n        elif self.state == MonitorState.MONITORING:\n            await self._update_progress()\n        elif self.state == MonitorState.COMPLETING:\n            await self._handle_completion()\n    \n    async def _detect_process(self) -> None:\n        self.state = MonitorState.DETECTING\n        process = self.detector.detect_mover()\n        \n        if process:\n            self.current_process = process\n            self.state = MonitorState.MONITORING\n            await self.notifier.send_start_notification(process)\n    \n    async def _update_progress(self) -> None:\n        if not self.detector.is_process_running(self.current_process.pid):\n            self.state = MonitorState.COMPLETING\n            return\n        \n        metrics = await self._calculate_progress()\n        await self.notifier.send_progress_update(metrics)\n```",
        "testStrategy": "Orchestrator testing:\n- Test state machine transitions\n- Test component coordination\n- Mock all dependencies\n- Test error recovery\n- Test graceful shutdown\n- Test event handling\n- Verify notification dispatch",
        "priority": "medium",
        "dependencies": [
          4,
          5,
          6,
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "State Machine Design",
            "description": "Design and implement a comprehensive state machine to manage the orchestrator's operational states and transitions",
            "dependencies": [],
            "details": "Create state definitions for idle, processing, error, recovery, and shutdown states. Define valid state transitions and guard conditions. Implement state persistence and restoration mechanisms. Design hierarchical states for complex workflows and ensure thread-safe state management.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Event Bus Implementation",
            "description": "Develop a robust event-driven communication system for inter-component messaging",
            "dependencies": [],
            "details": "Implement publish-subscribe pattern with topic-based routing. Create event serialization/deserialization mechanisms. Add event filtering, prioritization, and queuing capabilities. Ensure thread-safe event handling with proper error isolation and dead letter queue support.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Component Coordination Logic",
            "description": "Build the core orchestration logic that coordinates all subsystem components",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement component registration and discovery mechanisms. Create workflow execution engine with dependency resolution. Design resource allocation and scheduling algorithms. Build inter-component communication protocols and establish service health monitoring.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Error and Recovery Handling",
            "description": "Implement comprehensive error handling and automatic recovery mechanisms",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Design error classification and escalation strategies. Implement circuit breaker patterns for component failures. Create automatic retry mechanisms with exponential backoff. Build rollback and compensation transaction support with detailed error logging and alerting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Lifecycle Management",
            "description": "Develop complete lifecycle management for orchestrator and managed components",
            "dependencies": [
              1,
              3
            ],
            "details": "Implement graceful startup and shutdown procedures. Create component dependency ordering for initialization. Build health check and monitoring systems. Design configuration hot-reloading and version management with proper resource cleanup mechanisms.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Notification Integration",
            "description": "Integrate comprehensive notification system for orchestrator events and status updates",
            "dependencies": [
              2,
              4
            ],
            "details": "Implement multi-channel notification support (email, SMS, webhooks). Create notification templates and personalization. Build notification throttling and deduplication. Design escalation policies and integrate with external monitoring systems.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "TDD Test Suite for Orchestrator Behaviors",
            "description": "Develop comprehensive test-driven development suite covering all orchestrator behaviors",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Create unit tests for state machine transitions and event handling. Build integration tests for component coordination scenarios. Implement chaos engineering tests for failure scenarios. Design performance and load testing suites with mock component implementations and test data factories.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 11,
        "title": "Implement Integration Tests",
        "description": "Create comprehensive integration tests covering component interactions, full system workflows, and end-to-end scenarios with dry-run validation",
        "details": "Integration test implementation:\n1. Write tests/integration/scenarios/test_full_cycle.py:\n```python\nimport pytest\nfrom pathlib import Path\nimport tempfile\n\n@pytest.mark.integration\nasync def test_complete_monitoring_cycle():\n    \"\"\"Test full monitoring lifecycle from detection to completion\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        # Setup test environment\n        config = create_test_config(tmpdir)\n        \n        # Create mock mover process\n        mock_process = create_mock_mover_process()\n        \n        # Run monitoring cycle\n        app = Application(config)\n        await app.run_once()\n        \n        # Verify notifications sent\n        assert_notification_sent(\"start\")\n        assert_notification_sent(\"progress\")\n        assert_notification_sent(\"complete\")\n```\n\n2. Create end-to-end tests:\n```python\n@pytest.mark.e2e\ndef test_dry_run_mode():\n    \"\"\"Test dry-run doesn't send notifications\"\"\"\n    runner = CliRunner()\n    result = runner.invoke(cli, ['--dry-run', '--once'])\n    \n    assert result.exit_code == 0\n    assert \"DRY RUN\" in result.output\n    assert_no_notifications_sent()\n```\n\n3. Test failure scenarios:\n```python\ndef test_provider_failure_recovery():\n    \"\"\"Test system continues when provider fails\"\"\"\n    # Configure provider to fail\n    # Verify other providers still work\n    # Verify system doesn't crash\n    pass\n```\n\n4. Performance tests:\n```python\ndef test_large_filesystem_performance():\n    \"\"\"Test scanning performance with many files\"\"\"\n    # Create directory with 100k files\n    # Measure scan time\n    # Assert reasonable performance\n    pass\n```",
        "testStrategy": "Integration test strategy:\n- Use pytest markers for test categories\n- Create realistic test fixtures\n- Test component boundaries\n- Verify data flow between components\n- Test configuration changes\n- Measure performance metrics\n- Ensure 100% scenario coverage",
        "priority": "low",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Test Fixture Setup",
            "description": "Establish comprehensive test infrastructure including mock services, test databases, and isolated test environments for integration testing",
            "dependencies": [],
            "details": "Create reusable test fixtures, mock external dependencies, set up test data factories, configure isolated test environments, and establish teardown procedures to ensure clean test state between runs",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Full-Cycle Scenario Tests",
            "description": "Implement end-to-end integration tests covering complete user workflows and business processes across all system components",
            "dependencies": [
              1
            ],
            "details": "Design and execute comprehensive scenario tests that validate entire user journeys, cross-component interactions, data flow integrity, and business logic execution from start to finish",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Dry-Run Validation",
            "description": "Create validation tests that simulate system operations without executing actual changes to verify logic and flow correctness",
            "dependencies": [
              1
            ],
            "details": "Implement dry-run modes for critical operations, validate decision trees and conditional logic, test configuration changes, and ensure system behavior prediction accuracy without side effects",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Provider Failure Scenarios",
            "description": "Develop comprehensive failure simulation tests for external dependencies and service providers to validate system resilience",
            "dependencies": [
              1
            ],
            "details": "Create tests for network failures, service timeouts, API rate limiting, authentication failures, data corruption scenarios, and validate fallback mechanisms, retry logic, and error handling",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Performance and Stress Tests",
            "description": "Implement load testing and performance validation to ensure system scalability and reliability under various stress conditions",
            "dependencies": [
              1,
              2
            ],
            "details": "Design load tests for concurrent users, high-volume data processing, memory usage patterns, database performance, API response times, and system resource utilization under peak conditions",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Coverage and Reporting",
            "description": "Establish comprehensive test coverage measurement and reporting system to track integration test effectiveness and identify gaps",
            "dependencies": [
              2,
              3,
              4,
              5
            ],
            "details": "Implement code coverage tools, create integration test metrics dashboard, generate detailed test reports, track test execution trends, and establish coverage thresholds and quality gates",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 12,
        "title": "Add Telegram Provider",
        "description": "Implement the Telegram notification provider with Bot API integration, HTML/Markdown formatting, and multi-chat support",
        "details": "TDD Implementation:\n1. Write tests/unit/plugins/telegram/test_provider.py:\n```python\n@pytest.mark.asyncio\nasync def test_telegram_send_message():\n    provider = TelegramProvider({\n        \"bot_token\": \"test_token\",\n        \"chat_ids\": [\"123456\"]\n    })\n    \n    with patch('telegram.Bot.send_message') as mock_send:\n        mock_send.return_value = AsyncMock()\n        result = await provider.send_notification(test_message)\n        assert result is True\n```\n\n2. Implement Telegram provider:\n```python\nfrom telegram import Bot\nfrom telegram.constants import ParseMode\nfrom typing import List, Dict, Any\n\nclass TelegramProvider(NotificationProvider):\n    def __init__(self, config: Dict[str, Any]):\n        super().__init__(config)\n        self.bot_token = config['bot_token']\n        self.chat_ids = config['chat_ids']\n        self.parse_mode = config.get('parse_mode', 'HTML')\n        self.bot = Bot(token=self.bot_token)\n    \n    @with_retry(max_attempts=3)\n    async def send_notification(self, message: Message) -> bool:\n        text = self._format_message(message)\n        \n        tasks = [\n            self._send_to_chat(chat_id, text)\n            for chat_id in self.chat_ids\n        ]\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return all(r is True for r in results if not isinstance(r, Exception))\n    \n    async def _send_to_chat(self, chat_id: str, text: str) -> bool:\n        await self.bot.send_message(\n            chat_id=chat_id,\n            text=text,\n            parse_mode=self.parse_mode,\n            disable_web_page_preview=True\n        )\n        return True\n    \n    def _format_message(self, message: Message) -> str:\n        if self.parse_mode == 'HTML':\n            return self._format_html(message)\n        else:\n            return self._format_markdown(message)\n    \n    def _format_html(self, message: Message) -> str:\n        lines = [\n            f\"<b>{message.title}</b>\",\n            f\"{message.description}\"\n        ]\n        \n        if message.progress:\n            lines.append(f\"\\n<b>Progress:</b> {message.progress.percentage:.1f}%\")\n            if message.progress.etc_seconds:\n                lines.append(f\"<b>ETC:</b> {self._format_time(message.progress.etc_seconds)}\")\n        \n        return \"\\n\".join(lines)\n```",
        "testStrategy": "Telegram provider tests:\n- Mock telegram-bot library\n- Test message formatting (HTML/Markdown)\n- Test multi-chat delivery\n- Test API error handling\n- Test rate limiting\n- Verify message structure\n- Test connection failures",
        "priority": "low",
        "dependencies": [
          8
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Bot API Integration",
            "description": "Implement core Telegram Bot API integration with authentication, connection management, and basic message sending capabilities",
            "dependencies": [],
            "details": "Set up Telegram Bot API client, handle bot token authentication, implement connection pooling, create base message sending functionality, and establish secure API communication protocols",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Message Formatting (HTML/Markdown)",
            "description": "Develop message formatting system supporting both HTML and Markdown syntax for rich text notifications",
            "dependencies": [
              1
            ],
            "details": "Create formatters for HTML and Markdown parsing, implement text styling (bold, italic, code blocks), handle special characters escaping, support for links and mentions, and ensure proper rendering across different Telegram clients",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Multi-Chat Support",
            "description": "Implement functionality to send notifications to multiple Telegram chats, groups, and channels simultaneously",
            "dependencies": [
              1
            ],
            "details": "Design chat management system, support for individual users, groups, and channels, implement batch message sending, handle different chat types and permissions, and create configuration for multiple destination management",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Error and Rate Limit Handling",
            "description": "Implement comprehensive error handling and rate limiting mechanisms for reliable Telegram API interactions",
            "dependencies": [
              1,
              2,
              3
            ],
            "details": "Handle Telegram API errors (network, authentication, permissions), implement exponential backoff for rate limiting, create retry mechanisms, log error states, handle blocked bots scenarios, and ensure graceful degradation of service",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "TDD Test Coverage for All Features",
            "description": "Develop comprehensive test suite using Test-Driven Development approach covering all Telegram provider functionality",
            "dependencies": [
              1,
              2,
              3,
              4
            ],
            "details": "Write unit tests for API integration, message formatting, multi-chat support, and error handling, create integration tests with mock Telegram API, implement end-to-end testing scenarios, ensure 100% code coverage, and establish continuous testing pipeline",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-07-05T16:05:55.689Z",
      "updated": "2025-07-05T19:04:48.907Z",
      "description": "Tasks for master context"
    }
  }
}